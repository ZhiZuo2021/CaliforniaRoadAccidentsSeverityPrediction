{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Severity</th>\n",
       "      <th>Start_Lat</th>\n",
       "      <th>Start_Lng</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Side</th>\n",
       "      <th>County</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Wind_Direction</th>\n",
       "      <th>Weather_Condition</th>\n",
       "      <th>Amenity</th>\n",
       "      <th>...</th>\n",
       "      <th>Traffic_Calming</th>\n",
       "      <th>Traffic_Signal</th>\n",
       "      <th>Sunrise_Sunset</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Time_Duration</th>\n",
       "      <th>Severity3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.586512</td>\n",
       "      <td>0.216692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Solano</td>\n",
       "      <td>0.490460</td>\n",
       "      <td>VAR</td>\n",
       "      <td>Clear</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>Tue</td>\n",
       "      <td>0.112849</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.538525</td>\n",
       "      <td>0.231791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>0.491550</td>\n",
       "      <td>W</td>\n",
       "      <td>Clear</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>Tue</td>\n",
       "      <td>0.207880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.566540</td>\n",
       "      <td>0.233155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Contra Costa</td>\n",
       "      <td>0.489915</td>\n",
       "      <td>N</td>\n",
       "      <td>Clear</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>Tue</td>\n",
       "      <td>0.112849</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.507039</td>\n",
       "      <td>0.237035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>0.491187</td>\n",
       "      <td>N</td>\n",
       "      <td>Clear</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>Tue</td>\n",
       "      <td>0.112849</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.498200</td>\n",
       "      <td>0.249386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>0.491550</td>\n",
       "      <td>VAR</td>\n",
       "      <td>Clear</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>Tue</td>\n",
       "      <td>0.219759</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0.545908</td>\n",
       "      <td>0.249769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>0.490460</td>\n",
       "      <td>VAR</td>\n",
       "      <td>Clear</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>Tue</td>\n",
       "      <td>0.201941</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0.506410</td>\n",
       "      <td>0.253332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>0.491187</td>\n",
       "      <td>VAR</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>Tue</td>\n",
       "      <td>0.207880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0.547768</td>\n",
       "      <td>0.194857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>R</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>0.491187</td>\n",
       "      <td>NE</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>Tue</td>\n",
       "      <td>0.112849</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0.563530</td>\n",
       "      <td>0.220454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Contra Costa</td>\n",
       "      <td>0.489915</td>\n",
       "      <td>N</td>\n",
       "      <td>Clear</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>Tue</td>\n",
       "      <td>0.112849</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0.545841</td>\n",
       "      <td>0.260438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>0.490460</td>\n",
       "      <td>VAR</td>\n",
       "      <td>Clear</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>Tue</td>\n",
       "      <td>0.112849</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Severity  Start_Lat  Start_Lng  Distance Side         County  Pressure  \\\n",
       "0         3   0.586512   0.216692       0.0    R         Solano  0.490460   \n",
       "1         3   0.538525   0.231791       0.0    R        Alameda  0.491550   \n",
       "2         2   0.566540   0.233155       0.0    R   Contra Costa  0.489915   \n",
       "3         3   0.507039   0.237035       0.0    R    Santa Clara  0.491187   \n",
       "4         2   0.498200   0.249386       0.0    R    Santa Clara  0.491550   \n",
       "5         3   0.545908   0.249769       0.0    R        Alameda  0.490460   \n",
       "6         3   0.506410   0.253332       0.0    R    Santa Clara  0.491187   \n",
       "7         3   0.547768   0.194857       0.0    R  San Francisco  0.491187   \n",
       "8         2   0.563530   0.220454       0.0    R   Contra Costa  0.489915   \n",
       "9         3   0.545841   0.260438       0.0    R        Alameda  0.490460   \n",
       "\n",
       "  Wind_Direction Weather_Condition  Amenity  ...  Traffic_Calming  \\\n",
       "0            VAR             Clear    False  ...            False   \n",
       "1              W             Clear    False  ...            False   \n",
       "2              N             Clear    False  ...            False   \n",
       "3              N             Clear    False  ...            False   \n",
       "4            VAR             Clear    False  ...            False   \n",
       "5            VAR             Clear    False  ...            False   \n",
       "6            VAR            Cloudy    False  ...            False   \n",
       "7             NE            Cloudy    False  ...            False   \n",
       "8              N             Clear    False  ...            False   \n",
       "9            VAR             Clear    False  ...            False   \n",
       "\n",
       "   Traffic_Signal  Sunrise_Sunset  Year  Month       Day      Hour  Weekday  \\\n",
       "0           False             Day   0.0    Jun  0.666667  0.434783      Tue   \n",
       "1           False             Day   0.0    Jun  0.666667  0.434783      Tue   \n",
       "2           False             Day   0.0    Jun  0.666667  0.434783      Tue   \n",
       "3           False             Day   0.0    Jun  0.666667  0.434783      Tue   \n",
       "4           False             Day   0.0    Jun  0.666667  0.434783      Tue   \n",
       "5           False             Day   0.0    Jun  0.666667  0.434783      Tue   \n",
       "6           False             Day   0.0    Jun  0.666667  0.434783      Tue   \n",
       "7           False             Day   0.0    Jun  0.666667  0.434783      Tue   \n",
       "8           False             Day   0.0    Jun  0.666667  0.434783      Tue   \n",
       "9           False             Day   0.0    Jun  0.666667  0.434783      Tue   \n",
       "\n",
       "   Time_Duration  Severity3  \n",
       "0       0.112849          1  \n",
       "1       0.207880          1  \n",
       "2       0.112849          0  \n",
       "3       0.112849          1  \n",
       "4       0.219759          0  \n",
       "5       0.201941          1  \n",
       "6       0.207880          1  \n",
       "7       0.112849          1  \n",
       "8       0.112849          0  \n",
       "9       0.112849          1  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import CA data\n",
    "full_data = pd.read_csv(\"full_data.csv\")\n",
    "full_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "from sklearn.naive_bayes import CategoricalNB, BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics dictionary\n",
    "accuracy = dict()\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "f1 = dict()\n",
    "fpr = dict()\n",
    "tpr = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. set up training data, validation set and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00    80101\n",
       "1.00    71262\n",
       "0.25    51706\n",
       "0.75    10206\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Look at the year to make sure if we could use year to seperate the data\n",
    "full_data['Year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year  Severity3\n",
       "0.00  0            43382\n",
       "      1            36719\n",
       "0.25  0            27942\n",
       "      1            23764\n",
       "0.75  0             6686\n",
       "      1             3520\n",
       "1.00  0            50053\n",
       "      1            21209\n",
       "Name: Severity3, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = full_data.groupby(full_data['Year'])['Severity3'].value_counts()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Severity</th>\n",
       "      <th>Start_Lat</th>\n",
       "      <th>Start_Lng</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Side</th>\n",
       "      <th>County</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Wind_Direction</th>\n",
       "      <th>Weather_Condition</th>\n",
       "      <th>Amenity</th>\n",
       "      <th>...</th>\n",
       "      <th>Traffic_Calming</th>\n",
       "      <th>Traffic_Signal</th>\n",
       "      <th>Sunrise_Sunset</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Time_Duration</th>\n",
       "      <th>Severity3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.586512</td>\n",
       "      <td>0.216692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Solano</td>\n",
       "      <td>0.490460</td>\n",
       "      <td>VAR</td>\n",
       "      <td>Clear</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>Tue</td>\n",
       "      <td>0.112849</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.538525</td>\n",
       "      <td>0.231791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>0.491550</td>\n",
       "      <td>W</td>\n",
       "      <td>Clear</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>Tue</td>\n",
       "      <td>0.207880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.566540</td>\n",
       "      <td>0.233155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Contra Costa</td>\n",
       "      <td>0.489915</td>\n",
       "      <td>N</td>\n",
       "      <td>Clear</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>Tue</td>\n",
       "      <td>0.112849</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.507039</td>\n",
       "      <td>0.237035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>0.491187</td>\n",
       "      <td>N</td>\n",
       "      <td>Clear</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>Tue</td>\n",
       "      <td>0.112849</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.498200</td>\n",
       "      <td>0.249386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>R</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>0.491550</td>\n",
       "      <td>VAR</td>\n",
       "      <td>Clear</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>Tue</td>\n",
       "      <td>0.219759</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Severity  Start_Lat  Start_Lng  Distance Side        County  Pressure  \\\n",
       "0         3   0.586512   0.216692       0.0    R        Solano  0.490460   \n",
       "1         3   0.538525   0.231791       0.0    R       Alameda  0.491550   \n",
       "2         2   0.566540   0.233155       0.0    R  Contra Costa  0.489915   \n",
       "3         3   0.507039   0.237035       0.0    R   Santa Clara  0.491187   \n",
       "4         2   0.498200   0.249386       0.0    R   Santa Clara  0.491550   \n",
       "\n",
       "  Wind_Direction Weather_Condition  Amenity  ...  Traffic_Calming  \\\n",
       "0            VAR             Clear    False  ...            False   \n",
       "1              W             Clear    False  ...            False   \n",
       "2              N             Clear    False  ...            False   \n",
       "3              N             Clear    False  ...            False   \n",
       "4            VAR             Clear    False  ...            False   \n",
       "\n",
       "   Traffic_Signal  Sunrise_Sunset  Year  Month       Day      Hour  Weekday  \\\n",
       "0           False             Day   0.0    Jun  0.666667  0.434783      Tue   \n",
       "1           False             Day   0.0    Jun  0.666667  0.434783      Tue   \n",
       "2           False             Day   0.0    Jun  0.666667  0.434783      Tue   \n",
       "3           False             Day   0.0    Jun  0.666667  0.434783      Tue   \n",
       "4           False             Day   0.0    Jun  0.666667  0.434783      Tue   \n",
       "\n",
       "   Time_Duration  Severity3  \n",
       "0       0.112849          1  \n",
       "1       0.207880          1  \n",
       "2       0.112849          0  \n",
       "3       0.112849          1  \n",
       "4       0.219759          0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "features = ['Pressure','Distance','Start_Lng','Start_Lat','Year','Day','Hour','Time_Duration']\n",
    "full_data[features] = scaler.fit_transform(full_data[features])\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((149292, 113), (63983, 113))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. Split Trian set and Test set\n",
    "x = full_data.drop(['Year', 'Severity', 'Severity3'], axis=1)\n",
    "y = full_data['Severity3']\n",
    "\n",
    "x = x.replace([True, False], [1,0])\n",
    "\n",
    "category = ['Side','Wind_Direction','Weekday', 'County', 'Weather_Condition', 'Sunrise_Sunset', 'Month']\n",
    "x[category] = x[category].astype('category')\n",
    "x = pd.get_dummies(x, columns=category, drop_first=True)\n",
    "\n",
    "x_Train, x_test, y_Train, y_test = train_test_split(x, y, test_size=0.30, random_state=88)\n",
    "x_Train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((134362, 113), (14930, 113))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. Split train and validation set\n",
    "X_train, x_val, Y_train, y_val = train_test_split(x_Train, y_Train, test_size=0.10, random_state=88)\n",
    "X_train.shape,x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=1000), n_jobs=8,\n",
       "             param_grid={'C': array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]),\n",
       "                         'class_weight': ['balanced']},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "clf_base = LogisticRegression(max_iter = 1000)\n",
    "grid = {'C': 10.0 ** np.arange(-2, 3),\n",
    "        'class_weight': ['balanced']}\n",
    "clf_lr = GridSearchCV(clf_base, grid, cv=5, n_jobs=8, scoring='f1_macro')\n",
    "\n",
    "clf_lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters scores:\n",
      "{'C': 100.0, 'class_weight': 'balanced'}\n",
      "Train score: 0.7074505882625046\n",
      "Validation score: 0.7106317425328467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.64      0.73      8934\n",
      "           1       0.60      0.81      0.69      5996\n",
      "\n",
      "    accuracy                           0.71     14930\n",
      "   macro avg       0.72      0.73      0.71     14930\n",
      "weighted avg       0.74      0.71      0.71     14930\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters scores:\")\n",
    "print(clf_lr.best_params_)\n",
    "print(\"Train score:\", clf_lr.score(X_train, Y_train))\n",
    "print(\"Validation score:\", clf_lr.score(x_val, y_val))\n",
    "\n",
    "coef = clf_lr.best_estimator_.coef_\n",
    "intercept = clf_lr.best_estimator_.intercept_\n",
    "print (classification_report(y_val, clf_lr.predict(x_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100.0, class_weight='balanced', max_iter=1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the best model\n",
    "lr = LogisticRegression(**clf_lr.best_params_, max_iter = 1000)\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] max_depth=5, n_estimators=50 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... max_depth=5, n_estimators=50, total=  24.2s\n",
      "[CV] max_depth=5, n_estimators=50 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... max_depth=5, n_estimators=50, total=  25.4s\n",
      "[CV] max_depth=5, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=5, n_estimators=50, total=  24.6s\n",
      "[CV] max_depth=5, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=5, n_estimators=50, total=  24.6s\n",
      "[CV] max_depth=5, n_estimators=50 ....................................\n",
      "[CV] ..................... max_depth=5, n_estimators=50, total=  24.9s\n",
      "[CV] max_depth=5, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=100, total=  50.5s\n",
      "[CV] max_depth=5, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=100, total=  49.0s\n",
      "[CV] max_depth=5, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=100, total=  49.6s\n",
      "[CV] max_depth=5, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=100, total=  52.4s\n",
      "[CV] max_depth=5, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=100, total=  51.2s\n",
      "[CV] max_depth=5, n_estimators=200 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=200, total= 1.8min\n",
      "[CV] max_depth=5, n_estimators=200 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=200, total= 1.6min\n",
      "[CV] max_depth=5, n_estimators=200 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=200, total= 1.8min\n",
      "[CV] max_depth=5, n_estimators=200 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=200, total= 1.7min\n",
      "[CV] max_depth=5, n_estimators=200 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=200, total= 1.8min\n",
      "[CV] max_depth=5, n_estimators=500 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=500, total= 4.1min\n",
      "[CV] max_depth=5, n_estimators=500 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=500, total= 4.2min\n",
      "[CV] max_depth=5, n_estimators=500 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=500, total= 4.1min\n",
      "[CV] max_depth=5, n_estimators=500 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=500, total= 4.2min\n",
      "[CV] max_depth=5, n_estimators=500 ...................................\n",
      "[CV] .................... max_depth=5, n_estimators=500, total= 4.3min\n",
      "[CV] max_depth=10, n_estimators=50 ...................................\n",
      "[CV] .................... max_depth=10, n_estimators=50, total=  46.2s\n",
      "[CV] max_depth=10, n_estimators=50 ...................................\n",
      "[CV] .................... max_depth=10, n_estimators=50, total=  48.1s\n",
      "[CV] max_depth=10, n_estimators=50 ...................................\n",
      "[CV] .................... max_depth=10, n_estimators=50, total=  47.5s\n",
      "[CV] max_depth=10, n_estimators=50 ...................................\n",
      "[CV] .................... max_depth=10, n_estimators=50, total=  51.0s\n",
      "[CV] max_depth=10, n_estimators=50 ...................................\n",
      "[CV] .................... max_depth=10, n_estimators=50, total=  46.6s\n",
      "[CV] max_depth=10, n_estimators=100 ..................................\n",
      "[CV] ................... max_depth=10, n_estimators=100, total= 1.6min\n",
      "[CV] max_depth=10, n_estimators=100 ..................................\n",
      "[CV] ................... max_depth=10, n_estimators=100, total= 1.6min\n",
      "[CV] max_depth=10, n_estimators=100 ..................................\n",
      "[CV] ................... max_depth=10, n_estimators=100, total= 1.8min\n",
      "[CV] max_depth=10, n_estimators=100 ..................................\n",
      "[CV] ................... max_depth=10, n_estimators=100, total= 1.6min\n",
      "[CV] max_depth=10, n_estimators=100 ..................................\n",
      "[CV] ................... max_depth=10, n_estimators=100, total= 1.6min\n",
      "[CV] max_depth=10, n_estimators=200 ..................................\n",
      "[CV] ................... max_depth=10, n_estimators=200, total= 3.4min\n",
      "[CV] max_depth=10, n_estimators=200 ..................................\n",
      "[CV] ................... max_depth=10, n_estimators=200, total= 3.3min\n",
      "[CV] max_depth=10, n_estimators=200 ..................................\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "grid_values = {\"n_estimators\": [50, 100, 200, 500], \"max_depth\": [5, 10, 15, 30]}\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "rf2 = RandomForestRegressor() \n",
    "\n",
    "cv = KFold(n_splits=5,random_state=333,shuffle=True) \n",
    "grid = GridSearchCV(rf2, param_grid=grid_values, scoring='r2', cv=cv,verbose=2)\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "print('time:', round(toc-tic, 2),'s')\n",
    "print(\"Best parameters scores:\")\n",
    "print(grid.best_params_)\n",
    "print(\"Train score:\", grid.score(X_train, Y_train))\n",
    "print(\"Validation score:\", grid.score(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = time.time()\n",
    "\n",
    "print('time:', round(toc-tic, 2),'s')\n",
    "print(\"Best parameters scores:\")\n",
    "print(grid.best_params_)\n",
    "print(\"Train score:\", grid.score(X_train, Y_train))\n",
    "print(\"Validation score:\", grid.score(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = RandomForestClassifier(max_depth = 30, n_estimators = 500)\n",
    "\n",
    "print(\"Default scores:\")\n",
    "rf2.fit(X_train, Y_train)\n",
    "print(\"Train score:\", rf2.score(X_train, Y_train))\n",
    "print(\"Validation score:\", rf2.score(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Train score:\", gnb.score(X_train, Y_train))\n",
    "print(\"Validation score:\", gnb.score(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Set Performance\n",
    "\n",
    "### (a) Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = lr.predict(x_test)\n",
    "\n",
    "accuracy[\"Logistic Regression\"] = accuracy_score(y_test, y_pred)\n",
    "f1[\"Logistic Regression\"] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = lr.predict(x_test)\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "index = [\"Actual Severity 1\", \"Actual Severity 2\"]\n",
    "columns = [\"Predicted Severity 1\", \"Predicted Severity 2\"]\n",
    "conf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "Y = label_binarize(y_test, classes=[0, 1])\n",
    "Y = enc.fit_transform(Y).toarray()\n",
    "\n",
    "y_score = lr.predict_proba(x_test)\n",
    "print(y_score)\n",
    "precision[\"Logistic Regression\"], recall[\"Logistic Regression\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\n",
    "fpr[\"Logistic Regression\"], tpr[\"Logistic Regression\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n",
    "\n",
    "plt.step(recall[\"Logistic Regression\"], precision[\"Logistic Regression\"], where=\"post\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.title(\"PR Curve - Logisitc Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(fpr[\"Logistic Regression\"], tpr[\"Logistic Regression\"], where=\"post\")\n",
    "\n",
    "plt.title(\"ROC curve - Logistic Regression\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf2.predict(x_test)\n",
    "\n",
    "accuracy[\"Random Forest\"] = accuracy_score(y_test.values, y_pred)\n",
    "f1[\"Random Forest\"] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "index = [\"Actual Severity 1\", \"Actual Severity 2\"]\n",
    "columns = [\"Predicted Severity 1\", \"Predicted Severity 2\"]\n",
    "conf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "plt.title(\"Confusion Matrix - Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame(np.zeros((x_test.shape[1], 1)), columns=[\"importance\"], index=X_train.columns)\n",
    "\n",
    "importances.iloc[:,0] = rf2.feature_importances_\n",
    "\n",
    "importances = importances.sort_values(by=\"importance\", ascending=False)[:30]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(x=\"importance\", y=importances.index, data=importances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PR curve\n",
    "enc = OneHotEncoder()\n",
    "Y = label_binarize(y_test, classes=[0, 1])\n",
    "Y = enc.fit_transform(Y).toarray()\n",
    "\n",
    "y_score = rf2.predict_proba(x_test)\n",
    "\n",
    "precision[\"Random Forest\"], recall[\"Random Forest\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\n",
    "fpr[\"Random Forest\"], tpr[\"Random Forest\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n",
    "\n",
    "plt.step(recall[\"Random Forest\"], precision[\"Random Forest\"], where=\"post\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.title(\"PR Curve - Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(fpr[\"Random Forest\"], tpr[\"Random Forest\"], where=\"post\")\n",
    "\n",
    "plt.title(\"ROC curve - Random Forest\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred = rf2.predict(x_test)\n",
    "print(\"AUC score:\", roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(x_test)\n",
    "\n",
    "accuracy[\"Gaussian Naive Bayes\"] = accuracy_score(y_test, y_pred)\n",
    "f1[\"Gaussian Naive Bayes\"] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(x_test)\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "index = [\"Actual Severity 1\", \"Actual Severity 2\"]\n",
    "columns = [\"Predicted Severity 1\", \"Predicted Severity 2\"]\n",
    "conf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "plt.title(\"Confusion Matrix - Gaussian Naive Bayes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "Y = label_binarize(y_test, classes=[0, 1])\n",
    "Y = enc.fit_transform(Y).toarray()\n",
    "\n",
    "y_score = gnb.predict_proba(x_test)\n",
    "\n",
    "precision[\"Gaussian Naive Bayes\"], recall[\"Gaussian Naive Bayes\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\n",
    "fpr[\"Gaussian Naive Bayes\"], tpr[\"Gaussian Naive Bayes\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.step(recall[\"Gaussian Naive Bayes\"], precision[\"Gaussian Naive Bayes\"], where=\"post\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.title(\"PR Curve - Gaussian Naive Bayes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Accuracy on Validation set for each model\")\n",
    "sns.barplot(list(range(len(accuracy))), list(accuracy.values()))\n",
    "plt.xticks(range(len(accuracy)), labels=accuracy.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"F1 Score on Validation set for each model\")\n",
    "sns.barplot(list(range(len(f1))), list(f1.values()))\n",
    "plt.xticks(range(len(f1)), labels=f1.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "for key in f1.keys():\n",
    "    plt.step(recall[key], precision[key], where=\"post\", label=key)\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.title(\"PR curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "for key in f1.keys():\n",
    "    plt.step(fpr[key], tpr[key], where=\"post\", label=key)\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.title(\"ROC curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 ---- Add Population Density\n",
    "\n",
    "## 1. Adding Variable and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB, BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_popdensity = pd.read_csv(\"CA_pop_density.csv\")\n",
    "scaler = MinMaxScaler()\n",
    "features = ['Population Density']\n",
    "CA_popdensity[features] = scaler.fit_transform(CA_popdensity[features])\n",
    "CA_popdensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_new = pd.merge(full_data, CA_popdensity)\n",
    "full_data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Split Trian set and Test set\n",
    "x = full_data_new.drop(['Year', 'Severity', 'Severity3'], axis=1)\n",
    "y = full_data_new['Severity3']\n",
    "\n",
    "x = x.replace([True, False], [1,0])\n",
    "\n",
    "category = ['Side','Wind_Direction','Weekday', 'County', 'Weather_Condition', 'Sunrise_Sunset', 'Month']\n",
    "x[category] = x[category].astype('category')\n",
    "x = pd.get_dummies(x, columns=category, drop_first=True)\n",
    "\n",
    "x_Train, x_test, y_Train, y_test = train_test_split(x, y, test_size=0.30, random_state=88)\n",
    "x_Train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Split train and validation set\n",
    "X_train, x_val, Y_train, y_val = train_test_split(x_Train, y_Train, test_size=0.10, random_state=88)\n",
    "X_train.shape,x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "clf_base = LogisticRegression(max_iter = 1000)\n",
    "grid = {'C': 10.0 ** np.arange(-2, 3),\n",
    "        'class_weight': ['balanced']}\n",
    "clf_lr = GridSearchCV(clf_base, grid, cv=5, n_jobs=8, scoring='f1_macro')\n",
    "\n",
    "clf_lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters scores:\")\n",
    "print(clf_lr.best_params_)\n",
    "print(\"Train score:\", clf_lr.score(X_train, Y_train))\n",
    "print(\"Validation score:\", clf_lr.score(x_val, y_val))\n",
    "\n",
    "coef = clf_lr.best_estimator_.coef_\n",
    "intercept = clf_lr.best_estimator_.intercept_\n",
    "print (classification_report(y_val, clf_lr.predict(x_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the best model\n",
    "lr_population = LogisticRegression(**clf_lr.best_params_, max_iter = 1000)\n",
    "lr_population.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "grid_values = {\"n_estimators\": [50, 100, 200, 500], \"max_depth\": [5, 10, 15, 30]}\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "rf2 = RandomForestClassifier() \n",
    "\n",
    "cv = KFold(n_splits=5,random_state=333,shuffle=True) \n",
    "grid = GridSearchCV(rf2, param_grid=grid_values, scoring='r2', cv=cv,verbose=2)\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "print('time:', round(toc-tic, 2),'s')\n",
    "print(\"Best parameters scores:\")\n",
    "print(grid.best_params_)\n",
    "print(\"Train score:\", grid.score(X_train, Y_train))\n",
    "print(\"Validation score:\", grid.score(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_population = RandomForestClassifier(max_depth = 30, n_estimators = 500)\n",
    "\n",
    "print(\"Default scores:\")\n",
    "rf_population.fit(X_train, Y_train)\n",
    "print(\"Train score:\", rf_population.score(X_train, Y_train))\n",
    "print(\"Validation score:\", rf_population.score(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_population = GaussianNB()\n",
    "gnb_population.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Train score:\", gnb_population.score(X_train, Y_train))\n",
    "print(\"Validation score:\", gnb_population.score(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a). Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = lr_population.predict(x_test)\n",
    "\n",
    "accuracy[\"Logistic Regression\"] = accuracy_score(y_test, y_pred)\n",
    "f1[\"Logistic Regression\"] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = lr_population.predict(x_test)\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "index = [\"Actual Severity 1\", \"Actual Severity 2\"]\n",
    "columns = [\"Predicted Severity 1\", \"Predicted Severity 2\"]\n",
    "conf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(fpr[\"Logistic Regression\"], tpr[\"Logistic Regression\"], where=\"post\")\n",
    "\n",
    "plt.title(\"ROC curve - Logistic Regression (population)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b). Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_population.predict(x_test)\n",
    "\n",
    "accuracy[\"Random Forest\"] = accuracy_score(y_test.values, y_pred)\n",
    "f1[\"Random Forest\"] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "index = [\"Actual Severity 1\", \"Actual Severity 2\"]\n",
    "columns = [\"Predicted Severity 1\", \"Predicted Severity 2\"]\n",
    "conf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "plt.title(\"Confusion Matrix - Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame(np.zeros((x_test.shape[1], 1)), columns=[\"importance\"], index=X_train.columns)\n",
    "\n",
    "importances.iloc[:,0] = rf_population.feature_importances_\n",
    "\n",
    "importances = importances.sort_values(by=\"importance\", ascending=False)[:30]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(x=\"importance\", y=importances.index, data=importances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "Y = label_binarize(y_test, classes=[0, 1])\n",
    "Y = enc.fit_transform(Y).toarray()\n",
    "\n",
    "y_score = rf_population.predict_proba(x_test)\n",
    "\n",
    "precision[\"Random Forest\"], recall[\"Random Forest\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\n",
    "fpr[\"Random Forest\"], tpr[\"Random Forest\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n",
    "\n",
    "plt.step(recall[\"Random Forest\"], precision[\"Random Forest\"], where=\"post\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.title(\"PR Curve - Random Forest(population)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(fpr[\"Random Forest\"], tpr[\"Random Forest\"], where=\"post\")\n",
    "\n",
    "plt.title(\"ROC curve - Random Forest(population)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred = rf_population.predict(x_test)\n",
    "print(\"AUC score:\", roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c). Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb_population.predict(x_test)\n",
    "\n",
    "accuracy[\"Gaussian Naive Bayes\"] = accuracy_score(y_test, y_pred)\n",
    "f1[\"Gaussian Naive Bayes\"] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb_population.predict(x_test)\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "index = [\"Actual Severity 1\", \"Actual Severity 2\"]\n",
    "columns = [\"Predicted Severity 1\", \"Predicted Severity 2\"]\n",
    "conf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "plt.title(\"Confusion Matrix - Gaussian Naive Bayes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "Y = label_binarize(y_test, classes=[0, 1])\n",
    "Y = enc.fit_transform(Y).toarray()\n",
    "\n",
    "y_score = gnb_population.predict_proba(x_test)\n",
    "\n",
    "precision[\"Gaussian Naive Bayes\"], recall[\"Gaussian Naive Bayes\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\n",
    "fpr[\"Gaussian Naive Bayes\"], tpr[\"Gaussian Naive Bayes\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.step(recall[\"Gaussian Naive Bayes\"], precision[\"Gaussian Naive Bayes\"], where=\"post\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.title(\"PR Curve - Gaussian Naive Bayes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Accuracy on Validation set for each model\")\n",
    "sns.barplot(list(range(len(accuracy))), list(accuracy.values()))\n",
    "plt.xticks(range(len(accuracy)), labels=accuracy.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"F1 Score on Validation set for each model\")\n",
    "sns.barplot(list(range(len(f1))), list(f1.values()))\n",
    "plt.xticks(range(len(f1)), labels=f1.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in f1.keys():\n",
    "    plt.step(recall[key], precision[key], where=\"post\", label=key)\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.title(\"PR curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in f1.keys():\n",
    "    plt.step(fpr[key], tpr[key], where=\"post\", label=key)\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.title(\"ROC curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
